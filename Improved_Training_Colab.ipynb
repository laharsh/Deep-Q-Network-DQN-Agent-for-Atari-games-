{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Improved DQN Training for Atari Games\n",
        "\n",
        "This notebook runs the improved training with better hyperparameters to achieve actual learning!\n",
        "\n",
        "**Key Improvements:**\n",
        "- Slower epsilon decay (0.995 vs 0.99)\n",
        "- Lower learning rate (0.0001 vs 0.0005)\n",
        "- Larger replay buffer (100k vs 50k)\n",
        "- More episodes for proper learning\n",
        "\n",
        "**Expected Results:** Rewards should improve from -21 to positive values!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability and setup\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"üîç Checking GPU Setup...\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU detected!\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
        "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "    \n",
        "    # Test GPU allocation\n",
        "    test_tensor = torch.randn(1000, 1000).cuda()\n",
        "    print(f\"‚úÖ GPU test successful! Tensor on GPU: {test_tensor.device}\")\n",
        "    \n",
        "    # Set default device\n",
        "    torch.cuda.set_device(0)\n",
        "    print(f\"Default device set to: {torch.cuda.current_device()}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No GPU detected!\")\n",
        "    print(\"üîß Troubleshooting steps:\")\n",
        "    print(\"1. Go to Runtime > Change runtime type\")\n",
        "    print(\"2. Set Hardware accelerator to GPU\")\n",
        "    print(\"3. Choose T4 GPU (free) or A100/V100 (if available)\")\n",
        "    print(\"4. Click Save and restart runtime\")\n",
        "    print(\"5. Re-run this cell\")\n",
        "    \n",
        "    # Check if we're in Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        print(\"‚úÖ Running in Google Colab\")\n",
        "        print(\"üí° Make sure to enable GPU in runtime settings!\")\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è  Not running in Google Colab - GPU setup may be different\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install gymnasium[atari] ale-py opencv-python matplotlib tqdm\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installations\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "    import ale_py\n",
        "    import cv2\n",
        "    import matplotlib.pyplot as plt\n",
        "    from tqdm import tqdm\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    print(\"‚úÖ All packages installed successfully!\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import error: {e}\")\n",
        "    print(\"Please restart runtime and run the installation cell again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† DQN Implementation Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Atari Environment with Preprocessing\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import cv2\n",
        "from collections import deque\n",
        "\n",
        "# Register ALE environments\n",
        "try:\n",
        "    import ale_py\n",
        "    gym.register_envs(ale_py)\n",
        "except ImportError:\n",
        "    print(\"Warning: ale_py not installed\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not register ALE environments: {e}\")\n",
        "\n",
        "class AtariPreprocessor:\n",
        "    \"\"\"Handles frame preprocessing for Atari games.\"\"\"\n",
        "    \n",
        "    def __init__(self, frame_size=(84, 84), grayscale=True):\n",
        "        self.frame_size = frame_size\n",
        "        self.grayscale = grayscale\n",
        "    \n",
        "    def preprocess_frame(self, frame):\n",
        "        \"\"\"Preprocess a single frame.\"\"\"\n",
        "        if self.grayscale and len(frame.shape) == 3:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, self.frame_size, interpolation=cv2.INTER_AREA)\n",
        "        frame = frame.astype(np.float32) / 255.0\n",
        "        return frame\n",
        "\n",
        "class FrameStack:\n",
        "    \"\"\"Stack consecutive frames for temporal information.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_frames=4):\n",
        "        self.num_frames = num_frames\n",
        "        self.frames = deque(maxlen=num_frames)\n",
        "    \n",
        "    def reset(self, frame):\n",
        "        \"\"\"Initialize with the first frame.\"\"\"\n",
        "        for _ in range(self.num_frames):\n",
        "            self.frames.append(frame)\n",
        "        return self.get_state()\n",
        "    \n",
        "    def update(self, frame):\n",
        "        \"\"\"Add new frame and return stacked state.\"\"\"\n",
        "        self.frames.append(frame)\n",
        "        return self.get_state()\n",
        "    \n",
        "    def get_state(self):\n",
        "        \"\"\"Return current state as stacked frames.\"\"\"\n",
        "        return np.stack(list(self.frames), axis=0)\n",
        "\n",
        "class AtariEnvironment:\n",
        "    \"\"\"Complete Atari environment wrapper with preprocessing.\"\"\"\n",
        "    \n",
        "    def __init__(self, game_name=\"ALE/Pong-v5\", frame_size=(84, 84), num_frames=4, skip_frames=4, no_op_max=30):\n",
        "        self.game_name = game_name\n",
        "        self.skip_frames = skip_frames\n",
        "        self.no_op_max = no_op_max\n",
        "        \n",
        "        self.env = gym.make(game_name, render_mode=\"rgb_array\")\n",
        "        self.action_space = self.env.action_space\n",
        "        \n",
        "        self.preprocessor = AtariPreprocessor(frame_size)\n",
        "        self.frame_stack = FrameStack(num_frames)\n",
        "        \n",
        "        self.episode_reward = 0\n",
        "        self.episode_length = 0\n",
        "        self.lives = 0\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment and return initial state.\"\"\"\n",
        "        obs, info = self.env.reset()\n",
        "        self.episode_reward = 0\n",
        "        self.episode_length = 0\n",
        "        self.lives = info.get('lives', 0)\n",
        "        \n",
        "        for _ in range(np.random.randint(0, self.no_op_max)):\n",
        "            obs, _, terminated, truncated, info = self.env.step(0)\n",
        "            if terminated or truncated:\n",
        "                obs, info = self.env.reset()\n",
        "                self.lives = info.get('lives', 0)\n",
        "        \n",
        "        processed_frame = self.preprocessor.preprocess_frame(obs)\n",
        "        state = self.frame_stack.reset(processed_frame)\n",
        "        return state, info\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Execute action with frame skipping.\"\"\"\n",
        "        total_reward = 0\n",
        "        \n",
        "        for _ in range(self.skip_frames):\n",
        "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            \n",
        "            current_lives = info.get('lives', 0)\n",
        "            life_lost = (current_lives < self.lives) and (current_lives > 0)\n",
        "            self.lives = current_lives\n",
        "            \n",
        "            if terminated or truncated or life_lost:\n",
        "                break\n",
        "        \n",
        "        self.episode_reward += total_reward\n",
        "        self.episode_length += 1\n",
        "        \n",
        "        processed_frame = self.preprocessor.preprocess_frame(obs)\n",
        "        state = self.frame_stack.update(processed_frame)\n",
        "        \n",
        "        info['episode_reward'] = self.episode_reward\n",
        "        info['episode_length'] = self.episode_length\n",
        "        info['life_lost'] = life_lost if 'life_lost' in locals() else False\n",
        "        \n",
        "        return state, total_reward, terminated, truncated, info\n",
        "    \n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "    \n",
        "    def get_action_space_size(self):\n",
        "        return self.action_space.n\n",
        "    \n",
        "    def get_state_shape(self):\n",
        "        if len(self.frame_stack.frames) == 0:\n",
        "            dummy_frame = np.zeros(self.preprocessor.frame_size, dtype=np.float32)\n",
        "            dummy_state = np.stack([dummy_frame] * self.frame_stack.num_frames, axis=0)\n",
        "            return dummy_state.shape\n",
        "        else:\n",
        "            return self.frame_stack.get_state().shape\n",
        "\n",
        "print(\"‚úÖ Atari Environment classes loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DQN Agent Implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class DQNNetwork(nn.Module):\n",
        "    \"\"\"Deep Q-Network architecture for Atari games.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_shape, num_actions, hidden_size=512):\n",
        "        super(DQNNetwork, self).__init__()\n",
        "        \n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        \n",
        "        conv_out_size = self._get_conv_output_size()\n",
        "        \n",
        "        self.fc1 = nn.Linear(conv_out_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_actions)\n",
        "        \n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def _get_conv_output_size(self):\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, *self.input_shape)\n",
        "            x = F.relu(self.conv1(x))\n",
        "            x = F.relu(self.conv2(x))\n",
        "            x = F.relu(self.conv3(x))\n",
        "            return x.numel()\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 3:\n",
        "            x = x.unsqueeze(0)\n",
        "        \n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Experience replay buffer.\"\"\"\n",
        "    \n",
        "    def __init__(self, capacity=100000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.capacity = capacity\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        experience = Experience(state, action, reward, next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        experiences = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        states = torch.stack([torch.FloatTensor(e.state) for e in experiences])\n",
        "        actions = torch.LongTensor([e.action for e in experiences])\n",
        "        rewards = torch.FloatTensor([e.reward for e in experiences])\n",
        "        next_states = torch.stack([torch.FloatTensor(e.next_state) for e in experiences])\n",
        "        dones = torch.BoolTensor([e.done for e in experiences])\n",
        "        \n",
        "        return states, actions, rewards, next_states, dones\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"Deep Q-Network Agent with experience replay.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_shape, num_actions, learning_rate=0.0001, gamma=0.99,\n",
        "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
        "                 memory_size=100000, batch_size=32, target_update_freq=1000, device=None):\n",
        "        \n",
        "        self.state_shape = state_shape\n",
        "        self.num_actions = num_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update_freq = target_update_freq\n",
        "        \n",
        "        # Force GPU usage if available\n",
        "        if device is None:\n",
        "            if torch.cuda.is_available():\n",
        "                self.device = torch.device(\"cuda:0\")  # Explicitly use first GPU\n",
        "                torch.cuda.set_device(0)  # Set default GPU\n",
        "            else:\n",
        "                self.device = torch.device(\"cpu\")\n",
        "        else:\n",
        "            self.device = device\n",
        "        \n",
        "        print(f\"üöÄ DQN Agent using device: {self.device}\")\n",
        "        print(f\"üîß GPU Memory before model creation: {torch.cuda.memory_allocated()/1024**2:.1f} MB\" if torch.cuda.is_available() else \"\")\n",
        "        \n",
        "        # Create networks and move to device\n",
        "        self.q_network = DQNNetwork(state_shape, num_actions).to(self.device)\n",
        "        self.target_network = DQNNetwork(state_shape, num_actions).to(self.device)\n",
        "        \n",
        "        print(f\"üîß GPU Memory after model creation: {torch.cuda.memory_allocated()/1024**2:.1f} MB\" if torch.cuda.is_available() else \"\")\n",
        "        \n",
        "        self.update_target_network()\n",
        "        \n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.memory = ReplayBuffer(memory_size)\n",
        "        \n",
        "        self.steps_done = 0\n",
        "        self.training_step = 0\n",
        "        \n",
        "        # Test GPU usage\n",
        "        if torch.cuda.is_available():\n",
        "            test_input = torch.randn(1, *state_shape).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                test_output = self.q_network(test_input)\n",
        "            print(f\"‚úÖ GPU test successful! Input: {test_input.device}, Output: {test_output.device}\")\n",
        "    \n",
        "    def select_action(self, state, training=True):\n",
        "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.randrange(self.num_actions)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "                q_values = self.q_network(state_tensor)\n",
        "                return q_values.max(1)[1].item()\n",
        "    \n",
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store experience in replay buffer.\"\"\"\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "    \n",
        "    def train_step(self):\n",
        "        \"\"\"Perform one training step.\"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "        \n",
        "        states = states.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        next_states = next_states.to(self.device)\n",
        "        dones = dones.to(self.device)\n",
        "        \n",
        "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_network(next_states).max(1)[0]\n",
        "            target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
        "        \n",
        "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=10.0)\n",
        "        \n",
        "        self.optimizer.step()\n",
        "        \n",
        "        self.training_step += 1\n",
        "        \n",
        "        if self.training_step % self.target_update_freq == 0:\n",
        "            self.update_target_network()\n",
        "        \n",
        "        if self.epsilon > self.epsilon_end:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        \n",
        "        return loss.item()\n",
        "    \n",
        "    def update_target_network(self):\n",
        "        \"\"\"Copy weights to target network.\"\"\"\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "print(\"‚úÖ DQN Agent classes loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Improved Training Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Improved Training Function\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def improved_train(game_name=\"ALE/Pong-v5\", episodes=1000):\n",
        "    \"\"\"\n",
        "    Improved training with better hyperparameters for actual learning.\n",
        "    \"\"\"\n",
        "    print(f\"üöÄ Improved Training for Better Learning\")\n",
        "    print(f\"Game: {game_name}\")\n",
        "    print(f\"Episodes: {episodes}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # GPU monitoring\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"üîß Starting GPU Memory: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")\n",
        "        print(f\"üîß GPU Memory Cached: {torch.cuda.memory_reserved()/1024**2:.1f} MB\")\n",
        "    \n",
        "    # Create environment\n",
        "    print(\"üéÆ Creating environment...\")\n",
        "    env = AtariEnvironment(\n",
        "        game_name=game_name,\n",
        "        frame_size=(84, 84),\n",
        "        num_frames=4,\n",
        "        skip_frames=4\n",
        "    )\n",
        "    \n",
        "    state_shape = env.get_state_shape()\n",
        "    num_actions = env.get_action_space_size()\n",
        "    \n",
        "    print(f\"‚úÖ Environment created successfully\")\n",
        "    print(f\"   State shape: {state_shape}\")\n",
        "    print(f\"   Action space: {num_actions}\")\n",
        "    \n",
        "    # Create agent with IMPROVED settings\n",
        "    print(\"üß† Creating DQN agent with improved hyperparameters...\")\n",
        "    agent = DQNAgent(\n",
        "        state_shape=state_shape,\n",
        "        num_actions=num_actions,\n",
        "        learning_rate=0.0001,     # Lower learning rate for stability\n",
        "        gamma=0.99,\n",
        "        epsilon_start=1.0,\n",
        "        epsilon_end=0.01,\n",
        "        epsilon_decay=0.995,      # SLOWER decay - explore for much longer!\n",
        "        memory_size=100000,       # Larger buffer for better experience diversity\n",
        "        batch_size=32,\n",
        "        target_update_freq=1000   # Less frequent target updates for stability\n",
        "    )\n",
        "    \n",
        "    # Training metrics\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    average_rewards = []\n",
        "    losses = []\n",
        "    epsilon_values = []\n",
        "    \n",
        "    best_reward = float('-inf')\n",
        "    best_episode = 0\n",
        "    start_time = time.time()\n",
        "    \n",
        "    print(\"üéØ Starting IMPROVED training...\")\n",
        "    print(\"üîß Key improvements:\")\n",
        "    print(\"   ‚Ä¢ Slower epsilon decay (0.995 vs 0.99)\")\n",
        "    print(\"   ‚Ä¢ Lower learning rate (0.0001 vs 0.0005)\")\n",
        "    print(\"   ‚Ä¢ Larger replay buffer (100k vs 50k)\")\n",
        "    print(\"   ‚Ä¢ Less frequent target updates (1000 vs 500)\")\n",
        "    print(\"   ‚Ä¢ More episodes for proper learning\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    try:\n",
        "        for episode in tqdm(range(episodes), desc=\"Improved Training\"):\n",
        "            # Train one episode\n",
        "            state, _ = env.reset()\n",
        "            episode_reward = 0\n",
        "            episode_length = 0\n",
        "            episode_losses = []\n",
        "            \n",
        "            while True:\n",
        "                # Select and execute action\n",
        "                action = agent.select_action(state, training=True)\n",
        "                next_state, reward, terminated, truncated, info = env.step(action)\n",
        "                \n",
        "                # Store experience\n",
        "                done = terminated or truncated\n",
        "                agent.store_experience(state, action, reward, next_state, done)\n",
        "                \n",
        "                # Train agent (only if enough experiences)\n",
        "                if len(agent.memory) >= agent.batch_size:\n",
        "                    loss = agent.train_step()\n",
        "                    if loss is not None:\n",
        "                        episode_losses.append(loss)\n",
        "                \n",
        "                # Update state and statistics\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "                episode_length += 1\n",
        "                \n",
        "                if done:\n",
        "                    break\n",
        "            \n",
        "            # Record metrics\n",
        "            avg_loss = np.mean(episode_losses) if episode_losses else 0.0\n",
        "            \n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "            losses.append(avg_loss)\n",
        "            epsilon_values.append(agent.epsilon)\n",
        "            \n",
        "            # Calculate moving average (last 100 episodes)\n",
        "            if len(episode_rewards) >= 100:\n",
        "                recent_avg = np.mean(episode_rewards[-100:])\n",
        "            else:\n",
        "                recent_avg = np.mean(episode_rewards)\n",
        "            average_rewards.append(recent_avg)\n",
        "            \n",
        "            # Track best performance\n",
        "            if recent_avg > best_reward:\n",
        "                best_reward = recent_avg\n",
        "                best_episode = episode + 1\n",
        "            \n",
        "            # Progress reporting\n",
        "            if (episode + 1) % 50 == 0:\n",
        "                elapsed_time = time.time() - start_time\n",
        "                episodes_per_hour = (episode + 1) / (elapsed_time / 3600)\n",
        "                \n",
        "                gpu_info = \"\"\n",
        "                if torch.cuda.is_available():\n",
        "                    gpu_mem = torch.cuda.memory_allocated() / 1024**2\n",
        "                    gpu_info = f\" | GPU: {gpu_mem:.1f}MB\"\n",
        "                \n",
        "                print(f\"Episode {episode + 1:4d} | \"\n",
        "                      f\"Reward: {episode_reward:6.1f} | \"\n",
        "                      f\"Avg(100): {recent_avg:6.1f} | \"\n",
        "                      f\"Best: {best_reward:6.1f}@{best_episode:4d} | \"\n",
        "                      f\"Length: {episode_length:3d} | \"\n",
        "                      f\"Epsilon: {agent.epsilon:.3f} | \"\n",
        "                      f\"Speed: {episodes_per_hour:.1f}/hr{gpu_info}\")\n",
        "    \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n‚èπÔ∏è  Training interrupted by user\")\n",
        "    \n",
        "    finally:\n",
        "        # Calculate final statistics\n",
        "        total_time = time.time() - start_time\n",
        "        final_avg_reward = average_rewards[-1] if average_rewards else 0\n",
        "        improvement = final_avg_reward - episode_rewards[0] if episode_rewards else 0\n",
        "        episodes_per_hour = len(episode_rewards) / (total_time / 3600)\n",
        "        \n",
        "        # Final results\n",
        "        final_results = {\n",
        "            'total_episodes': len(episode_rewards),\n",
        "            'total_time_minutes': total_time / 60,\n",
        "            'best_avg_reward': best_reward,\n",
        "            'best_episode': best_episode,\n",
        "            'final_avg_reward': final_avg_reward,\n",
        "            'improvement': improvement,\n",
        "            'episodes_per_hour': episodes_per_hour,\n",
        "            'final_epsilon': agent.epsilon\n",
        "        }\n",
        "        \n",
        "        # Print final summary\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"üéâ IMPROVED TRAINING COMPLETED!\")\n",
        "        print(\"=\" * 50)\n",
        "        print(\"üìä Final Results:\")\n",
        "        print(f\"   Total Episodes: {final_results['total_episodes']}\")\n",
        "        print(f\"   Total Time: {final_results['total_time_minutes']:.1f} minutes\")\n",
        "        print(f\"   Best Average Reward: {final_results['best_avg_reward']:.2f} (Episode {final_results['best_episode']})\")\n",
        "        print(f\"   Final Average Reward: {final_results['final_avg_reward']:.2f}\")\n",
        "        print(f\"   Improvement: {final_results['improvement']:.2f}\")\n",
        "        print(f\"   Episodes per Hour: {final_results['episodes_per_hour']:.1f}\")\n",
        "        print(f\"   Final Epsilon: {final_results['final_epsilon']:.3f}\")\n",
        "        \n",
        "        # Learning assessment\n",
        "        if final_results['improvement'] > 5:\n",
        "            print(\"üéØ EXCELLENT LEARNING! Significant improvement achieved!\")\n",
        "        elif final_results['improvement'] > 1:\n",
        "            print(\"‚úÖ GOOD LEARNING! Some improvement achieved!\")\n",
        "        elif final_results['improvement'] > 0:\n",
        "            print(\"‚ö†Ô∏è  MINIMAL LEARNING! Very small improvement.\")\n",
        "        else:\n",
        "            print(\"‚ùå NO LEARNING! Consider adjusting hyperparameters further.\")\n",
        "        \n",
        "        env.close()\n",
        "        \n",
        "        return {\n",
        "            'results': final_results,\n",
        "            'episode_rewards': episode_rewards,\n",
        "            'average_rewards': average_rewards,\n",
        "            'losses': losses,\n",
        "            'epsilon_values': epsilon_values\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Improved training function loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß GPU Monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Memory Monitoring Function\n",
        "def monitor_gpu():\n",
        "    \"\"\"Monitor GPU memory usage.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**2  # MB\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**2    # MB\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1024**2  # MB\n",
        "        \n",
        "        print(f\"üîß GPU Memory Status:\")\n",
        "        print(f\"   Allocated: {allocated:.1f} MB ({allocated/total*100:.1f}%)\")\n",
        "        print(f\"   Reserved:  {reserved:.1f} MB ({reserved/total*100:.1f}%)\")\n",
        "        print(f\"   Total:     {total:.1f} MB\")\n",
        "        print(f\"   Available: {total-reserved:.1f} MB\")\n",
        "        \n",
        "        # Clear cache if needed\n",
        "        if allocated > total * 0.9:  # If using >90% of memory\n",
        "            print(\"‚ö†Ô∏è  High GPU memory usage! Clearing cache...\")\n",
        "            torch.cuda.empty_cache()\n",
        "    else:\n",
        "        print(\"‚ùå No GPU available for monitoring\")\n",
        "\n",
        "# Run GPU monitoring\n",
        "monitor_gpu()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéÆ Run Training\n",
        "\n",
        "Choose your training configuration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick Test (10 episodes) - to verify everything works\n",
        "print(\"üß™ Running Quick Test (10 episodes)...\")\n",
        "results = improved_train(game_name=\"ALE/Pong-v5\", episodes=10)\n",
        "print(\"‚úÖ Quick test completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Medium Training (100 episodes) - to see some learning\n",
        "print(\"üöÄ Running Medium Training (100 episodes)...\")\n",
        "results = improved_train(game_name=\"ALE/Pong-v5\", episodes=100)\n",
        "print(\"‚úÖ Medium training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full Training (500 episodes) - for proper learning\n",
        "print(\"üéØ Running Full Training (500 episodes)...\")\n",
        "results = improved_train(game_name=\"ALE/Pong-v5\", episodes=500)\n",
        "print(\"‚úÖ Full training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training results\n",
        "if 'results' in locals():\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Improved Training Results', fontsize=16)\n",
        "    \n",
        "    # Episode rewards\n",
        "    axes[0, 0].plot(results['episode_rewards'], alpha=0.6, label='Episode Reward', color='lightblue')\n",
        "    axes[0, 0].plot(results['average_rewards'], label='Moving Average (100)', linewidth=2, color='blue')\n",
        "    axes[0, 0].set_xlabel('Episode')\n",
        "    axes[0, 0].set_ylabel('Reward')\n",
        "    axes[0, 0].set_title('Episode Rewards')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Training loss\n",
        "    axes[0, 1].plot(results['losses'], color='orange', alpha=0.7)\n",
        "    axes[0, 1].set_xlabel('Episode')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].set_title('Training Loss')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Epsilon decay\n",
        "    axes[1, 0].plot(results['epsilon_values'], color='purple', alpha=0.7)\n",
        "    axes[1, 0].set_xlabel('Episode')\n",
        "    axes[1, 0].set_ylabel('Epsilon')\n",
        "    axes[1, 0].set_title('Exploration Rate')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Reward distribution\n",
        "    axes[1, 1].hist(results['episode_rewards'], bins=50, alpha=0.7, color='green')\n",
        "    axes[1, 1].set_xlabel('Reward')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].set_title('Reward Distribution')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"\\nüìä Training Summary:\")\n",
        "    print(f\"   Initial Reward: {results['episode_rewards'][0]:.1f}\")\n",
        "    print(f\"   Final Reward: {results['episode_rewards'][-1]:.1f}\")\n",
        "    print(f\"   Best Average: {results['results']['best_avg_reward']:.1f}\")\n",
        "    print(f\"   Improvement: {results['results']['improvement']:.1f}\")\n",
        "else:\n",
        "    print(\"‚ùå No training results available. Run a training cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Custom Training\n",
        "\n",
        "Run your own training with custom parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Training Configuration\n",
        "GAME_NAME = \"ALE/Pong-v5\"  # Try: ALE/Breakout-v5, ALE/SpaceInvaders-v5\n",
        "EPISODES = 200  # Adjust as needed\n",
        "\n",
        "print(f\"üéÆ Custom Training: {GAME_NAME} for {EPISODES} episodes\")\n",
        "custom_results = improved_train(game_name=GAME_NAME, episodes=EPISODES)\n",
        "print(\"‚úÖ Custom training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° Tips for Better Results\n",
        "\n",
        "1. **Start with Pong** - It's the easiest Atari game to learn\n",
        "2. **Watch for Improvement** - Rewards should go from -21 to positive values\n",
        "3. **Be Patient** - DQN needs 200+ episodes to show meaningful learning\n",
        "4. **Monitor Epsilon** - Should decay slowly from 1.0 to 0.01\n",
        "5. **Check GPU Usage** - Make sure you're using GPU for faster training\n",
        "\n",
        "## üéØ Expected Learning Progression:\n",
        "\n",
        "- **Episodes 1-50**: Random play, -21 rewards\n",
        "- **Episodes 50-150**: Occasional ball returns, -15 to -10 rewards\n",
        "- **Episodes 150-300**: Consistent returns, -5 to +5 rewards\n",
        "- **Episodes 300+**: Strategic play, +5 to +20 rewards\n",
        "\n",
        "## üö® Important Notes:\n",
        "\n",
        "- **GPU Required**: Enable GPU in Runtime > Change runtime type > GPU\n",
        "- **Restart Runtime**: If you encounter import errors, restart runtime and run all cells\n",
        "- **Save Results**: Download plots and results before runtime expires\n",
        "- **Monitor Progress**: Watch the progress bars and reward improvements\n",
        "\n",
        "## üîß GPU Troubleshooting:\n",
        "\n",
        "If you see 0% GPU usage:\n",
        "\n",
        "1. **Check Runtime Settings**: Runtime > Change runtime type > GPU (T4 recommended)\n",
        "2. **Restart Runtime**: Runtime > Restart runtime (after changing GPU settings)\n",
        "3. **Re-run Setup Cells**: Run the GPU detection cell again after restart\n",
        "4. **Verify CUDA**: Make sure PyTorch CUDA version matches Colab's CUDA\n",
        "5. **Check GPU Monitoring**: Use the GPU monitoring cell to verify memory usage\n",
        "6. **Force GPU**: The updated code now explicitly uses `cuda:0` device\n",
        "\n",
        "**Expected GPU Usage**: Should show 200-500MB GPU memory during training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
